package com.gromozeka.shared.domain

import kotlinx.serialization.Serializable
import kotlin.jvm.JvmInline
import kotlin.time.Instant

/**
 * Token usage statistics for a single AI model invocation.
 *
 * Tracks token consumption for cost estimation and performance analysis.
 * Each record corresponds to one AI request/response cycle (turn).
 *
 * Token types:
 * - Prompt tokens: Input sent to model (user messages, context, system prompt)
 * - Completion tokens: Output generated by model (assistant response)
 * - Cache creation tokens: Tokens used to create prompt cache (first use)
 * - Cache read tokens: Tokens read from prompt cache (subsequent uses, much cheaper)
 * - Thinking tokens: Tokens used for extended thinking mode (Claude feature)
 *
 * Prompt caching significantly reduces costs for repeated context.
 * Cache read tokens are typically 90% cheaper than regular prompt tokens.
 *
 * This is an immutable record - statistics cannot be modified after creation.
 *
 * @property id unique statistics record identifier
 * @property threadId conversation thread this turn belongs to
 * @property turnNumber sequence number within thread (1, 2, 3, ...)
 * @property timestamp when this turn was recorded (immutable)
 * @property promptTokens tokens in input prompt (context + user message)
 * @property completionTokens tokens in AI response
 * @property cacheCreationTokens tokens used to create prompt cache (0 if cache hit or no caching)
 * @property cacheReadTokens tokens read from cache (0 if cache miss or no caching)
 * @property thinkingTokens tokens used for extended thinking (0 for models without thinking mode)
 * @property modelId AI model identifier (e.g., "claude-3-5-sonnet-20241022")
 */
@Serializable
data class TokenUsageStatistics(
    val id: Id,
    val threadId: Conversation.Thread.Id,
    val turnNumber: Int,
    val timestamp: Instant,
    val promptTokens: Int,
    val completionTokens: Int,
    val cacheCreationTokens: Int = 0,
    val cacheReadTokens: Int = 0,
    val thinkingTokens: Int = 0,
    val modelId: String
) {
    @Serializable
    @JvmInline
    value class Id(val value: String)

    /**
     * Total tokens consumed (prompt + completion).
     *
     * Does NOT include cache tokens in total as they are already counted
     * in promptTokens (either as creation or read).
     */
    val totalTokens: Int
        get() = promptTokens + completionTokens

    /**
     * Aggregated token statistics for an entire thread.
     *
     * Summarizes token usage across all turns in a conversation thread.
     * Useful for cost estimation and performance monitoring.
     *
     * @property totalPromptTokens sum of all prompt tokens
     * @property totalCompletionTokens sum of all completion tokens
     * @property totalCacheReadTokens sum of all cache read tokens (cost savings indicator)
     * @property totalThinkingTokens sum of all thinking tokens
     * @property lastCallTokens tokens used in most recent turn (null if no turns)
     * @property recentCalls recent turn statistics for trend analysis
     * @property currentContextSize estimated current context size in tokens (null if unknown)
     * @property modelId model identifier (null if mixed models used)
     */
    @Serializable
    data class ThreadTotals(
        val totalPromptTokens: Int,
        val totalCompletionTokens: Int,
        val totalCacheReadTokens: Int,
        val totalThinkingTokens: Int,
        val lastCallTokens: Int?,
        val recentCalls: List<TokenUsageStatistics>,
        val currentContextSize: Int? = null,
        val modelId: String? = null
    ) {
        /**
         * Total tokens consumed across all turns.
         */
        val totalTokens: Int
            get() = totalPromptTokens + totalCompletionTokens
    }
}
